{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate, UpSampling2D, AveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Lambda, Flatten, Dense\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import PIL\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from tensorflow import keras\n",
    "from IPython.display import clear_output as cls\n",
    "from fr_utils import *\n",
    "import win32com.client as wincl\n",
    "from multiprocessing import Pool\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING = 50\n",
    "ready_to_detect_identity = True\n",
    "windows10_voice_interface = wincl.Dispatch(\"SAPI.SpVoice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## For collect image dataset on webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Create directories for each person\n",
    "# person_name = \"Boom\"\n",
    "# os.makedirs(f\"dataset/{person_name}\", exist_ok=True)\n",
    "\n",
    "# # Initialize webcam\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "# img_count = 0\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "#     for (x, y, w, h) in faces:\n",
    "#         img_count += 1\n",
    "#         face = frame[y:y + h, x:x + w]\n",
    "#         cv2.imwrite(f\"dataset/{person_name}/{person_name}_{img_count+400}.jpg\", face)\n",
    "#         cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "#     cv2.imshow(\"Face Capture\", frame)\n",
    "\n",
    "    \n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q') or img_count >= 100:  # Collect 100 images\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define the image size (input size for FRmodel)\n",
    "IMG_SIZE = (160, 160)\n",
    "\n",
    "# Prepare lists to hold the images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Dataset path\n",
    "dataset_path = './dataset/'\n",
    "\n",
    "# Loop through each person in the dataset\n",
    "for person in os.listdir(dataset_path):\n",
    "    person_path = os.path.join(dataset_path, person)\n",
    "    \n",
    "    if os.path.isdir(person_path):\n",
    "        for img_name in os.listdir(person_path):\n",
    "            img_path = os.path.join(person_path, img_name)\n",
    "            \n",
    "            # Load image\n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "            if img is not None:\n",
    "                # Resize to 160x160\n",
    "                img = cv2.resize(img, IMG_SIZE)\n",
    "                \n",
    "                # Normalize pixel values\n",
    "                img = img / 255.0\n",
    "                \n",
    "                # Append the image and its label\n",
    "                images.append(img)\n",
    "                labels.append(person)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Encode the labels (e.g., 'Auu', 'Phupha', 'Boom')\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.3):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss as defined by formula (3)\n",
    "    \n",
    "    Arguments:\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
    "            positive -- the encodings for the positive images, of shape (None, 128)\n",
    "            negative -- the encodings for the negative images, of shape (None, 128)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    \n",
    "    # Step 1: Compute the (encoding) distance between the anchor and the positive, you will need to sum over axis=-1\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=-1)\n",
    "    # Step 2: Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=-1)\n",
    "    # Step 3: subtract the two previous distances and add alpha.\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n",
    "    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0.0))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\eiei\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:184: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from inception_blocks_v2 import *\n",
    "\n",
    "FRmodel = faceRecoModel(input_shape=(3, 160, 160))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Model\n",
    "# from keras.optimizers import Adam\n",
    "\n",
    "# # # Compile the model\n",
    "# FRmodel.compile(optimizer=Adam(learning_rate=0.0001), loss=triplet_loss, metrics = ['accuracy'])\n",
    "\n",
    "# # # Train the model (this is a simplified example)\n",
    "# FRmodel.fit(tf.transpose(images, perm=[0, 3, 1, 2]), labels_encoded, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FRmodel.save_weights('FRmodel.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model weights from a file\n",
    "FRmodel.load_weights('FRmodel.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FRmodel.evaluate(tf.transpose(images, perm=[0, 3, 1, 2]), labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 128)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FRmodel.predict(np.expand_dims(tf.transpose(images[0], perm=[2, 0, 1]), axis = 0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def img_to_encoding(image, model):\n",
    "#     \"\"\"\n",
    "#     Converts an image to an encoding using the given model.\n",
    "\n",
    "#     Arguments:\n",
    "#     image -- a numpy array of the image\n",
    "#     model -- Keras model used to encode the image\n",
    "\n",
    "#     Returns:\n",
    "#     encoding -- the encoded representation of the image\n",
    "#     \"\"\"\n",
    "#     # If the image is a path, load it; otherwise, assume it's already a NumPy array\n",
    "#     if isinstance(image, str):\n",
    "#         img = tf.keras.preprocessing.image.load_img(image, target_size=(160, 160))\n",
    "#         img = np.array(img) / 255.0\n",
    "#     else:\n",
    "#         img = np.around(image / 255.0, decimals=12)\n",
    "\n",
    "#     x_train = np.expand_dims(img, axis=0)\n",
    "#     x_train = np.transpose(x_train, (0, 3, 2, 1))\n",
    "#     encoding = model.predict(x_train)\n",
    "#     return encoding\n",
    "\n",
    "def img_to_encoding(image, model):\n",
    "    if isinstance(image, str):\n",
    "        img = tf.keras.preprocessing.image.load_img(image, target_size=(160, 160))\n",
    "        img = np.array(img) / 255.0\n",
    "    else:\n",
    "        img = np.around(image / 255.0, decimals=12)\n",
    "\n",
    "    # Ensure the input shape is correct (160, 160, 3)\n",
    "    if img.shape != (160, 160, 3):\n",
    "        img = cv2.resize(img, (160, 160))  # Resize the image if needed\n",
    "\n",
    "    x_train = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    x_train = np.transpose(x_train, (0, 3, 2, 1))\n",
    "    encoding = model.predict(x_train)\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_encodings(encoding1: np.ndarray, encoding2: np.ndarray, threshold: float) -> tuple:\n",
    "    \n",
    "    \"\"\"\n",
    "    Compare two face encodings using Euclidean distance and a threshold.\n",
    "    If the distance is less than the threshold, the encodings are considered a match.\n",
    "    \n",
    "    Parameters:\n",
    "    encoding1 (np.ndarray): Face encoding 1\n",
    "    encoding2 (np.ndarray): Face encoding 2\n",
    "    threshold (float): Threshold for considering two encodings as a match\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple with two elements - a binary flag indicating whether the encodings are a match (1 if match, 0 otherwise), \n",
    "           and the distance between the encodings\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the Euclidean distance between the two encodings\n",
    "    dist = np.linalg.norm(encoding1 - encoding2)\n",
    "    \n",
    "    # Check if the distance is less than the threshold\n",
    "    if dist < threshold:\n",
    "        \n",
    "        # Return a match and the distance\n",
    "        return 1, dist\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Return no match and the distance\n",
    "        return 0, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 768ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 768ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 775ms/step\n"
     ]
    }
   ],
   "source": [
    "database = {}\n",
    "database[\"Phupha\"] = img_to_encoding(\"dataset/Phupha/Phupha_149.jpg\", FRmodel)\n",
    "database[\"Boom\"] = img_to_encoding(\"dataset/Boom/Boom_422.jpg\", FRmodel)\n",
    "database[\"Auu\"] = img_to_encoding(\"dataset/Auu/Auu_337.jpg\", FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def who_is_it(image, database, model):\n",
    "    \"\"\"\n",
    "    Implements face recognition for the happy house by finding who is the person on the image_path image.\n",
    "    \n",
    "    Arguments:\n",
    "    image_path -- path to an image\n",
    "    database -- database containing image encodings along with the name of the person on the image\n",
    "    model -- your Inception model instance in Keras\n",
    "    \n",
    "    Returns:\n",
    "    min_dist -- the minimum distance between image_path encoding and the encodings from the database\n",
    "    identity -- string, the name prediction for the person on image_path\n",
    "    \"\"\"\n",
    "    encoding = img_to_encoding(image, model)\n",
    "    \n",
    "    min_dist = 100\n",
    "    identity = None\n",
    "    \n",
    "    # Loop over the database dictionary's names and encodings.\n",
    "    for (name, db_enc) in database.items():\n",
    "        \n",
    "        # Compute L2 distance between the target \"encoding\" and the current \"emb\" from the database.\n",
    "        dist = np.linalg.norm(db_enc - encoding)\n",
    "\n",
    "        print('distance for %s is %s' %(name, dist))\n",
    "\n",
    "        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "    \n",
    "    if min_dist > 0.7:\n",
    "        return None\n",
    "    else:\n",
    "        return str(identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# who_is_it(\"dataset/Auu/Auu_300.jpg\", database, FRmodel)\n",
    "# who_is_it(\"dataset/Boom/Boom_500.jpg\", database, FRmodel)\n",
    "# who_is_it(\"dataset/Phupha/Phupha_200.jpg\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def webcam_face_recognizer(database):\n",
    "    \"\"\"\n",
    "    Runs a loop that extracts images from the computer's webcam and determines whether or not\n",
    "    it contains the face of a person in our database.\n",
    "\n",
    "    If it contains a face, an audio message will be played welcoming the user.\n",
    "    If not, the program will process the next frame from the webcam\n",
    "    \"\"\"\n",
    "    global ready_to_detect_identity\n",
    "\n",
    "    cv2.namedWindow(\"preview\")\n",
    "    vc = cv2.VideoCapture(0)\n",
    "\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    # face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    while vc.isOpened():\n",
    "        _, frame = vc.read()\n",
    "        img = frame\n",
    "\n",
    "        # We do not want to detect a new identity while the program is in the process of identifying another person\n",
    "        if ready_to_detect_identity:\n",
    "            img = process_frame(img, frame, face_cascade)   \n",
    "        \n",
    "        key = cv2.waitKey(100)\n",
    "        cv2.imshow(\"preview\", img)\n",
    "\n",
    "        if key == 27: # exit on ESC\n",
    "            break\n",
    "    cv2.destroyWindow(\"preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(img, frame, face_cascade):\n",
    "    \"\"\"\n",
    "    Determine whether the current frame contains the faces of people from our database\n",
    "    \"\"\"\n",
    "    global ready_to_detect_identity\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # Check if faces are being detected\n",
    "    if len(faces) == 0:\n",
    "        print(\"No face detected\")\n",
    "    else:\n",
    "        print(f\"{len(faces)} face(s) detected\")\n",
    "\n",
    "    # Loop through all the faces detected and determine whether or not they are in the database\n",
    "    identities = []\n",
    "    for (x, y, w, h) in faces:\n",
    "        x1 = x-PADDING\n",
    "        y1 = y-PADDING\n",
    "        x2 = x+w+PADDING\n",
    "        y2 = y+h+PADDING\n",
    "\n",
    "        img = cv2.rectangle(frame,(x1, y1),(x2, y2),(255,0,0),2)\n",
    "\n",
    "        identity = find_identity(frame, x1, y1, x2, y2)\n",
    "\n",
    "        if identity is not None:\n",
    "            print(f\"Identity found: {identity}\")\n",
    "            identities.append(identity)\n",
    "            # Put the identity text on the frame\n",
    "            cv2.putText(img, identity, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        else:\n",
    "            print(\"No identity found for the detected face\")\n",
    "\n",
    "    if identities:\n",
    "        cv2.imwrite('example.png', img)\n",
    "        ready_to_detect_identity = False\n",
    "        pool = Pool(processes=1)\n",
    "        # Process in a separate thread so the camera doesn't freeze\n",
    "        pool.apply_async(welcome_users, [identities])\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_identity(frame, x1, y1, x2, y2):\n",
    "    \"\"\"\n",
    "    Determine whether the face contained within the bounding box exists in our database\n",
    "\n",
    "    x1,y1_____________\n",
    "    |                 |\n",
    "    |                 |\n",
    "    |_________________x2,y2\n",
    "\n",
    "    \"\"\"\n",
    "    height, width, channels = frame.shape\n",
    "    # The padding is necessary since the OpenCV face detector creates the bounding box around the face and not the head\n",
    "    part_image = frame[max(0, y1):min(height, y2), max(0, x1):min(width, x2)]\n",
    "\n",
    "    # Check if the part image is valid\n",
    "    if part_image.size == 0:\n",
    "        print(\"Invalid image region\")\n",
    "        return None\n",
    "\n",
    "    # Get the identity\n",
    "    identity = who_is_it(part_image, database, FRmodel)\n",
    "    \n",
    "    return identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def welcome_users(identities):\n",
    "    \"\"\" Outputs a welcome audio message to the users \"\"\"\n",
    "    global ready_to_detect_identity\n",
    "    welcome_message = 'Welcome '\n",
    "\n",
    "    if len(identities) == 1:\n",
    "        welcome_message += '%s, have a nice day.' % identities[0]\n",
    "    else:\n",
    "        for identity_id in range(len(identities)-1):\n",
    "            welcome_message += '%s, ' % identities[identity_id]\n",
    "        welcome_message += 'and %s, ' % identities[-1]\n",
    "        welcome_message += 'have a nice day!'\n",
    "\n",
    "    windows10_voice_interface.Speak(welcome_message)\n",
    "\n",
    "    # Allow the program to start detecting identities again\n",
    "    ready_to_detect_identity = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No face detected\n",
      "No face detected\n",
      "1 face(s) detected\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 813ms/step\n",
      "distance for Phupha is 0.102593005\n",
      "distance for Boom is 0.10027188\n",
      "distance for Auu is 0.119707\n",
      "Identity found: Boom\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    webcam_face_recognizer(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
